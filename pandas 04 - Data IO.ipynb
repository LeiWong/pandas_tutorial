{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas 04 - Data IO\n",
    "\n",
    "by Nova@Douban\n",
    "\n",
    "The video record of this session is here: https://zoom.us/recording/share/p2BRTD-McWEb51tNf6S1SBBBw9FDO3GJdL4JrbaG-uiwIumekTziMw\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "When we load data into Python, we have a demand: using a unified and powerful tool to read / write data.\n",
    "\n",
    "According to the [latest Pandas doc](http://pandas.pydata.org/pandas-docs/stable/io.html), Pandas supports reading and supporting these commonly-used file format: \n",
    "\n",
    "1. CSV, \n",
    "2. JSON, \n",
    "3. HTML, \n",
    "4. Local clipboard, \n",
    "5. MS Excel, \n",
    "6. HDF5 Format, \n",
    "7. Feather Format, \n",
    "8. Msgpack, \n",
    "9. Stata, \n",
    "10. SAS, \n",
    "11. Python Pickle Format, \n",
    "12. SQL, \n",
    "13. Google Big Query. \n",
    "\n",
    "If we visualize these data formats, we can have a clearer idea:\n",
    "\n",
    "![pandoc file conversion map](http://acepor.github.io/images/pandas_relations.png)\n",
    "\n",
    "\n",
    "__Advantages__\n",
    "\n",
    "Using Pandas as a unified IO tool has two main advantages:\n",
    "\n",
    "1. Pandas IO tools provide a significant performance increase when reading or writing data.\n",
    "2. Pandas has very detailed document, so the learning curse is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.1 CSV\n",
    "\n",
    "CSV (comma-separated-value) format is one of the most common formats in data processing. It is easy for both human and machine to read.\n",
    "\n",
    "### 4.1.1 Read CSV to DataFrame\n",
    "\n",
    "`pd.read_csv(in_file, quoting=0, sep=',', engine='c')`\n",
    "\n",
    "1. `quoting` is to tell which quotation convention the data uses.\n",
    "\n",
    "2. If the `sep` set as `None` and `engine` as 'python', this function will automatically sniff the delimiter.\n",
    "\n",
    "3. `c` engine is much faster (at least 50%) than `python` engine, but `python` engine supports more features\n",
    "\n",
    "4. `usecols` to select columns in order to reduce memory usage.\n",
    "\n",
    "### 4.1.2 Write DataFrame to CSV\n",
    "\n",
    "`pd.DataFrame.to_csv(out_file, header=True, index=False)`\n",
    "\n",
    "1. If we want to keep header and index, we can set `header` and `index` as `True`, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:31.180777Z",
     "start_time": "2019-01-06T11:08:30.639715Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%load_ext memory_profiler\n",
    "in_csv = '../data/first_count.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:31.598577Z",
     "start_time": "2019-01-06T11:08:31.184404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 108.51 MiB, increment: 34.71 MiB\n",
      "CPU times: user 210 ms, sys: 75 ms, total: 285 ms\n",
      "Wall time: 407 ms\n"
     ]
    }
   ],
   "source": [
    "%time %memit first_name = pd.read_csv(in_csv, engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:33.201264Z",
     "start_time": "2019-01-06T11:08:31.604767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 173.65 MiB, increment: 67.99 MiB\n",
      "CPU times: user 1.35 s, sys: 105 ms, total: 1.45 s\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%time %memit first_name = pd.read_csv(in_csv, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:33.236176Z",
     "start_time": "2019-01-06T11:08:33.204439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 321792 entries, 0 to 321791\n",
      "Data columns (total 3 columns):\n",
      "MSISDN_SEG    321792 non-null int64\n",
      "AREA_CODE     321792 non-null int64\n",
      "ASP           321792 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 7.4 MB\n"
     ]
    }
   ],
   "source": [
    "first_name.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:34.941681Z",
     "start_time": "2019-01-06T11:08:33.239012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 125.99 MiB, increment: 14.78 MiB\n",
      "CPU times: user 1.48 s, sys: 59.4 ms, total: 1.54 s\n",
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "out_csv = '../data/first_count.csv'\n",
    "%time %memit first_name.to_csv(out_csv, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2 JSON\n",
    "\n",
    "JSON has gain more popularity recently. It has more controls on data, but it is not very human-friendly. JSON has different orients: `split`, `records`, `index`, `columns` or `values`. \n",
    "\n",
    "_Screenshot of JSON columns file_\n",
    "\n",
    "<img src=\"../image/json_columns.png\">\n",
    "\n",
    "_Screenshot of JSON index file_\n",
    "\n",
    "<img src=\"../image/json_index.png\">\n",
    "\n",
    "_Screenshot of JSON split file_\n",
    "\n",
    "<img src=\"../image/json_split.png\">\n",
    "\n",
    "_Screenshot of JSON values file\n",
    "\n",
    "<img src=\"../image/json_values.png\">\n",
    "\n",
    "_Screenshot of JSON lines file\n",
    "\n",
    "<img src=\"../image/json_lines.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Read JSON to DataFrame\n",
    "\n",
    "Because it has a number of orients, it is quite easy to get confused. Therefore, when we use Pandas to read a JSON file, we have to specify the orient. \n",
    "\n",
    "__Moreover, it the file is line-based, we can set `lines` as `True`.__\n",
    "\n",
    "`pd.read_json(in_file, orient='records', lines=False)`\n",
    "\n",
    "### 4.2.2 Write  DataFrame to JSON\n",
    "\n",
    "Always save Json as `lines`\n",
    "\n",
    "`pd.DataFrame.to_json(out_file, orient='records', lines=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:35.173713Z",
     "start_time": "2019-01-06T11:08:34.945172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 123.61 MiB, increment: 0.23 MiB\n",
      "CPU times: user 68.2 ms, sys: 37.2 ms, total: 105 ms\n",
      "Wall time: 219 ms\n"
     ]
    }
   ],
   "source": [
    "in_json = '../data/nasdaq.json'\n",
    "%time %memit nasdaq = pd.read_json(in_json, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:35.547768Z",
     "start_time": "2019-01-06T11:08:35.177696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 180.29 MiB, increment: 56.68 MiB\n",
      "CPU times: user 180 ms, sys: 62.8 ms, total: 243 ms\n",
      "Wall time: 361 ms\n"
     ]
    }
   ],
   "source": [
    "out_json = '../data/first_count.json'\n",
    "%time %memit first_name.to_json(out_json, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Swiss knife for JSON\n",
    "\n",
    "Sometimes, a JSON file can be very nasty, and we just couldn't figure out how to read it. Luckily, `pandas` has a Swiss knife for this task -- `pd.io.json.json_normalize`.\n",
    "\n",
    "The example in ths screenshot is an Unserialized JSON file generated by `request` lib. This file cannot be read by `pd.read_json`, so we used `pd.io.json.json_normalize` instead. \n",
    "\n",
    "<img src=\"../image/json_screenshot.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:35.801634Z",
     "start_time": "2019-01-06T11:08:35.551407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 175.99 MiB, increment: 0.04 MiB\n",
      "CPU times: user 55.8 ms, sys: 30.7 ms, total: 86.5 ms\n",
      "Wall time: 199 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_geoloc.lat</th>\n",
       "      <th>_geoloc.lng</th>\n",
       "      <th>_highlightResult.region.hasc</th>\n",
       "      <th>advertiser.avatar_url</th>\n",
       "      <th>advertiser.category</th>\n",
       "      <th>advertiser.id</th>\n",
       "      <th>advertiser.name</th>\n",
       "      <th>advertiser.phone</th>\n",
       "      <th>advertiser.phone_full.phone</th>\n",
       "      <th>advertiser.phone_full.status</th>\n",
       "      <th>...</th>\n",
       "      <th>rental_yield</th>\n",
       "      <th>sale_price</th>\n",
       "      <th>slug</th>\n",
       "      <th>station</th>\n",
       "      <th>title.en</th>\n",
       "      <th>title.ja</th>\n",
       "      <th>title.ru</th>\n",
       "      <th>title.th</th>\n",
       "      <th>token</th>\n",
       "      <th>transaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.723916</td>\n",
       "      <td>100.566902</td>\n",
       "      <td>[{'value': 'TH', 'matchLevel': 'none', 'matche...</td>\n",
       "      <td>https://files.hipcdn.com/avatars/53faa8bd93164...</td>\n",
       "      <td>Agent</td>\n",
       "      <td>559636cd70726f2451000094</td>\n",
       "      <td>Findbangkokroom.com</td>\n",
       "      <td>099-095-5...</td>\n",
       "      <td>099-095-5535</td>\n",
       "      <td>ok</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>bangkok-condo</td>\n",
       "      <td>[509ea305d2af11286e000ace, 509ea305d2af11286e0...</td>\n",
       "      <td>For Rent 5 Beds Condo in Khlong Toei, Bangkok,...</td>\n",
       "      <td>For Rent 5 Beds コンド in Khlong Toei, Bangkok, T...</td>\n",
       "      <td>В аренду: Кондо с 5 спальнями в районе Khlong ...</td>\n",
       "      <td>ให้เช่า คอนโด 5 ห้องนอน คลองเตย กรุงเทพฯ</td>\n",
       "      <td>AAFBALPC</td>\n",
       "      <td>[rent]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _geoloc.lat  _geoloc.lng  \\\n",
       "0    13.723916   100.566902   \n",
       "\n",
       "                        _highlightResult.region.hasc  \\\n",
       "0  [{'value': 'TH', 'matchLevel': 'none', 'matche...   \n",
       "\n",
       "                               advertiser.avatar_url advertiser.category  \\\n",
       "0  https://files.hipcdn.com/avatars/53faa8bd93164...               Agent   \n",
       "\n",
       "              advertiser.id      advertiser.name advertiser.phone  \\\n",
       "0  559636cd70726f2451000094  Findbangkokroom.com     099-095-5...   \n",
       "\n",
       "  advertiser.phone_full.phone advertiser.phone_full.status     ...      \\\n",
       "0                099-095-5535                           ok     ...       \n",
       "\n",
       "   rental_yield  sale_price           slug  \\\n",
       "0          None        None  bangkok-condo   \n",
       "\n",
       "                                             station  \\\n",
       "0  [509ea305d2af11286e000ace, 509ea305d2af11286e0...   \n",
       "\n",
       "                                            title.en  \\\n",
       "0  For Rent 5 Beds Condo in Khlong Toei, Bangkok,...   \n",
       "\n",
       "                                            title.ja  \\\n",
       "0  For Rent 5 Beds コンド in Khlong Toei, Bangkok, T...   \n",
       "\n",
       "                                            title.ru  \\\n",
       "0  В аренду: Кондо с 5 спальнями в районе Khlong ...   \n",
       "\n",
       "                                   title.th     token transaction  \n",
       "0  ให้เช่า คอนโด 5 ห้องนอน คลองเตย กรุงเทพฯ  AAFBALPC      [rent]  \n",
       "\n",
       "[1 rows x 75 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "\n",
    "in_json = '../data/AAFBALPC.json'\n",
    "\n",
    "def convert_json(in_file):\n",
    "    with open(in_file) as json_data:\n",
    "        data = json.load(json_data)\n",
    "        del data['formatted']\n",
    "        df = json_normalize(data)\n",
    "        return df\n",
    "\n",
    "%time %memit df = convert_json(in_json)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.3 HDF5\n",
    "\n",
    "HDF5 is a unique file format. We can include multiple other-format files into a single HDF5 file, and used a key to index them. Therefore, we can save space and reading speed of multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:35.812367Z",
     "start_time": "2019-01-06T11:08:35.805229Z"
    }
   },
   "outputs": [],
   "source": [
    "def hdf2df(in_hdf, hdf_keys):\n",
    "    \"\"\"\n",
    "    Read a hdf5 file and return all dfs\n",
    "    :param in_hdf: a hdf5 file\n",
    "    :param hdf_keys:\n",
    "    :return a dict of df\n",
    "    \"\"\"\n",
    "    return {i: pd.read_hdf(in_hdf, i) for i in hdf_keys}\n",
    "\n",
    "\n",
    "def df2hdf(out_hdf, dfs, hdf_keys, mode='a'):\n",
    "    \"\"\"\n",
    "    Store single or multiple dfs to one hdf5 file\n",
    "    :param dfs: single of multiple dfs\n",
    "    :param out_hdf: the output file\n",
    "    :param hdf_keys: [key for hdf]\n",
    "    \"\"\"\n",
    "    for j, k in zip(dfs, hdf_keys):\n",
    "        j.to_hdf(out_hdf, k, table=True, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.4 MySQL\n",
    "\n",
    "MySQL is one of the most popular databases, and `pandas` can easily read the data from it with the help of another Python library `sqlalchemy`.\n",
    "\n",
    "### 4.4.1 Read MySQL table to DataFrame\n",
    "\n",
    "1. use `sqlalchemy` to make a MySQL connection.\n",
    "\n",
    "2. give a SQL query to pandas, and query from the created connection.\n",
    "\n",
    "\n",
    "### 4.4.2 Write DataFrame to MySQL\n",
    "\n",
    "1. make MySQL connection,\n",
    "2. write DataFrame to MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:35.940990Z",
     "start_time": "2019-01-06T11:08:35.817447Z"
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "def connect_db(host):\n",
    "    return create_engine(host)\n",
    "\n",
    "def mysql2df(sql, con):\n",
    "    \"\"\"\n",
    "    pull data from SQl to dataframe\n",
    "    :param sql: sql query\n",
    "    :param con: sql connection\n",
    "    :return: df\n",
    "    \"\"\"\n",
    "    return pd.read_sql_query(sql=sql, con=con)\n",
    "    \n",
    "    \n",
    "def df2mysql(df, table_name, con, if_exist):\n",
    "    \"\"\"\n",
    "    save df to sql\n",
    "    :param df:\n",
    "    :param table_name: sql table name\n",
    "    :param con: sql connection\n",
    "    :param if_exist: append if existed\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df.to_sql(table_name, con, if_exists=if_exist, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.5 Excel\n",
    "\n",
    "Excel is one of the most common data file formats, and pandas can handle it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:36.413028Z",
     "start_time": "2019-01-06T11:08:35.943521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 189.64 MiB, increment: 9.37 MiB\n",
      "CPU times: user 245 ms, sys: 66.8 ms, total: 312 ms\n",
      "Wall time: 458 ms\n"
     ]
    }
   ],
   "source": [
    "def df2excel(df, out_excel):\n",
    "    writer = pd.ExcelWriter(out_excel)\n",
    "    df.to_excel(writer,'Sheet1')\n",
    "    writer.save()\n",
    "\n",
    "out_excel = '../data/test.xlsx'\n",
    "%time %memit df2excel(nasdaq, out_excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.6 Benchmark of reading / writing large files with pandas\n",
    "\n",
    "This benchmark was run on a Google full name count file: 2 columns * 25,891,901 rows.\n",
    "\n",
    "<img src=\"../image/io_benchmark.png\">\n",
    "\n",
    "---\n",
    "\n",
    "The conclusion is that __Parquet__ uses the least time to read and write, requires least time to read, and the output size is the smallest, although it requires the most time to write.\n",
    "\n",
    "### 4.6.1 Converting between DataFrame and Arrow table\n",
    "\n",
    "> Apache Arrow is a cross-language development platform for in-memory data. \n",
    "\n",
    "> It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. \n",
    "\n",
    "> It also provides computational libraries and zero-copy streaming messaging and interprocess communication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:36.937389Z",
     "start_time": "2019-01-06T11:08:36.418748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 192.28 MiB, increment: 0.39 MiB\n",
      "CPU times: user 58.8 ms, sys: 46.3 ms, total: 105 ms\n",
      "Wall time: 224 ms\n",
      "peak memory: 184.56 MiB, increment: -7.65 MiB\n",
      "CPU times: user 67.5 ms, sys: 87 ms, total: 154 ms\n",
      "Wall time: 246 ms\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "%time %memit table = pa.Table.from_pandas(first_name)\n",
    "%time %memit df_new = table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4.6.2 Fatest way to write a DataFrame to disk\n",
    "\n",
    "write a DataFrame to parquet without compression with `pyarrow` lib\n",
    "\n",
    "1. convert DataFrame to Arrow table\n",
    "2. write table to Parquet on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:38.676088Z",
     "start_time": "2019-01-06T11:08:36.942137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 170.10 MiB, increment: 2.57 MiB\n",
      "CPU times: user 1.47 s, sys: 71.8 ms, total: 1.54 s\n",
      "Wall time: 1.73 s\n"
     ]
    }
   ],
   "source": [
    "%time %memit first_name.to_csv(out_csv, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:39.051013Z",
     "start_time": "2019-01-06T11:08:38.679447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 150.27 MiB, increment: 6.93 MiB\n",
      "CPU times: user 161 ms, sys: 69.6 ms, total: 231 ms\n",
      "Wall time: 351 ms\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "out_pq = '../data/test.pq'\n",
    "\n",
    "def df_parquet(df, out_pq):\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, out_pq, compression='none')\n",
    "\n",
    "\n",
    "%time %memit df_parquet(first_name, out_pq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4.6.3 Fatest way to read a file to a DataFrame\n",
    "\n",
    "read an uncompressed parquet to a DataFrame with `pyarrow` lib.\n",
    "\n",
    "1. read Parquet file to Arrow table\n",
    "2. convert table to pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:39.510881Z",
     "start_time": "2019-01-06T11:08:39.055755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 160.75 MiB, increment: 14.10 MiB\n",
      "CPU times: user 230 ms, sys: 87.7 ms, total: 318 ms\n",
      "Wall time: 446 ms\n"
     ]
    }
   ],
   "source": [
    "%time %memit first_name = pd.read_csv(in_csv, engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T11:08:39.771942Z",
     "start_time": "2019-01-06T11:08:39.519861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 162.25 MiB, increment: 13.16 MiB\n",
      "CPU times: user 97.6 ms, sys: 90.9 ms, total: 188 ms\n",
      "Wall time: 239 ms\n"
     ]
    }
   ],
   "source": [
    "def parquet_df(in_pq):\n",
    "    table = pq.read_table(in_pq)\n",
    "    return table.to_pandas()\n",
    "\n",
    "in_pq = '../data/test.pq'\n",
    "\n",
    "%time %memit parquet_df(in_pq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Exercise\n",
    "\n",
    "1. Read the comprehensive introduction of Pandas IO tools [here](http://pandas.pydata.org/pandas-docs/stable/io.html).\n",
    "\n",
    "2. Find out how to read an Excel file to pandas DataFrame.\n",
    "\n",
    "3. Test all solutions in the benchmark table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
